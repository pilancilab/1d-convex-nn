{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef864841",
   "metadata": {},
   "source": [
    "# Functions to train neural nets with non-convex training problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744bd9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618401e",
   "metadata": {},
   "source": [
    "For $L=2,4$, we initialize a subset of the neurons to a solution of the min-norm version of the Lasso problem. The rest of the neurons are initialized randomly.\n",
    "\n",
    "Train a neural net by running ```nonconvex_nn()```. Plot the results by running ```plotresults()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "249b68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "            \n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    \n",
    "class DeepNarrowNet1D(nn.Module): #ez\n",
    "    def __init__(self, m_L, L, par):\n",
    "        super().__init__()\n",
    "        \n",
    "        #generic layers (we don't use)\n",
    "        self.one2one = nn.Linear(1, 1, bias=True)\n",
    "        self.m2m = nn.Linear(m_L, m_L, bias=True)\n",
    "        \n",
    "        #layers with names (we use)\n",
    "        self.one2one1 = nn.Linear(1, 1, bias=True)\n",
    "        self.one2one2 = nn.Linear(1, 1, bias=True)\n",
    "        self.one2m = nn.Linear(1, m_L, bias=True)\n",
    "        self.last = nn.Linear(m_L, 1, bias=True)\n",
    "        self.L=L\n",
    "        self.par=par\n",
    "        self.m_L=m_L\n",
    "        \n",
    "        self.mask = torch.eye(m_L, dtype=bool)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): #can I write this in matrix form\n",
    "        \n",
    "        if self.par:\n",
    "            x = self.one2m(x)\n",
    "            for i in range(self.L-2):  \n",
    "                self.m2m.weight.data *= self.mask\n",
    "                #self.m2m.bias.data *= self.mask\n",
    "                x = F.relu(self.m2m(x))\n",
    "            x = self.last(x) \n",
    "        else:\n",
    "            for i in range(self.L-2): \n",
    "                #x = F.relu(self.one2one(x))\n",
    "                if i==0:\n",
    "                    x = F.relu(self.one2one1(x)) #name layers so we can initialize them easily \n",
    "                elif i==1:\n",
    "                    x = F.relu(self.one2one2(x)) \n",
    "            x = F.relu(self.one2m(x))\n",
    "            x = self.last(x)      \n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "def print_weights(module): #this function is from: https://wandb.ai/wandb_fc/tips/reports/How-to-Initialize-Weights-in-PyTorch--VmlldzoxNjcwOTg1\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print('weights = ', + module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                print('bias = ', + module.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562e4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(L, opt, move0toneg1):\n",
    "    min_num_nuerons = 1 #default\n",
    "    \n",
    "    if opt:\n",
    "        X = np.array([0,4,5,7,10,11]).astype(np.float64).reshape(-1,1) \n",
    "        y = np.array((0,0,0,2, 3, 3)).astype(np.float64) \n",
    "        seed= 7\n",
    "        β=(1e-7)/L\n",
    "        if L==2:\n",
    "            m_L = 100\n",
    "            num_epochs = int(1e3)\n",
    "            min_num_nuerons = 3\n",
    "        if L==3:\n",
    "            m_L = 500 \n",
    "            num_epochs = int(1e5)\n",
    "        if L==4:\n",
    "            m_L = 100\n",
    "            num_epochs = int(1e3) \n",
    "    else:\n",
    "        X = np.array([0,2,6,7]).astype(np.float64).reshape(-1,1) \n",
    "        y = np.array([0,0,3,3]).astype(np.float64) \n",
    "        seed= 8\n",
    "        β=(1e-8)/L\n",
    "        if L==2:\n",
    "            m_L = 100\n",
    "            num_epochs = int(1e6)\n",
    "            min_num_nuerons = 2\n",
    "        if L==3:\n",
    "            m_L = 100 \n",
    "            num_epochs = int(1e3)\n",
    "        if L==4:           \n",
    "            m_L = 100\n",
    "            if move0toneg1:\n",
    "                num_epochs = int(1.5e4) \n",
    "            else:\n",
    "                num_epochs = int(1e3)\n",
    "\n",
    "    if move0toneg1:\n",
    "        X[0]=-1\n",
    "        \n",
    "    setup_dic={'X':X, 'y':y, 'seed':seed, 'β':β, 'm_L': m_L, 'num_epochs':num_epochs, 'min_num_nuerons':min_num_nuerons}\n",
    "        \n",
    "    return setup_dic\n",
    "\n",
    "def prepTrainTestData(X,y,batchsize_frac=1):\n",
    "    n,d=X.shape\n",
    "    batch_size= int(n*batchsize_frac) #n/2 can also help\n",
    "    ds_train = PrepareData(X=X, y=y)\n",
    "    ds_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    ds_test = PrepareData(X=X, y=y)\n",
    "    ds_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return ds_train, ds_test\n",
    "\n",
    "def initnn(net, L, opt, initkinkat4, move0toneg1, min_num_nuerons, debug=False, custom_init=True, initrescale = True, init_normal=False, std=0.01):\n",
    "    \n",
    "    if debug:\n",
    "        init_perfect = True\n",
    "    else:\n",
    "        init_perfect = False\n",
    "        \n",
    "        \n",
    "    #initialize a minimal subset of neurons to generate an optimal solution, and initialize all other neurons randomly.\n",
    "    if custom_init:\n",
    "        if init_perfect: #rest of optimal net\n",
    "            net.one2m.weight.data.fill_(0.0)\n",
    "            net.one2m.bias.data.fill_(0.0)\n",
    "            net.last.weight.data.fill_(0.0)\n",
    "            net.last.bias.data.fill_(0.0)\n",
    "        if init_normal:\n",
    "            net.one2m.weight.data.normal_(mean=0.0,std=std)\n",
    "            net.one2m.bias.data.normal_(mean=0.0,std=std)\n",
    "            net.last.weight.data.normal_(mean=0.0,std=std)\n",
    "            net.last.bias.data.normal_(mean=0.0,std=std)\n",
    "\n",
    "        if L==2: #b is kink points, w is weights\n",
    "            if opt:  \n",
    "                α = np.array((1, -2/3, -1/3))\n",
    "                w = np.array((1,1,1))\n",
    "                b = np.array((-5,-7,-10))  \n",
    "            else:\n",
    "                w = np.array((1,1))\n",
    "                b = np.array((-2,-5))\n",
    "                slope = -3/(b[1]-b[0])\n",
    "                α = np.array((slope, -1*slope))\n",
    "\n",
    "            γ = np.abs(α)**(1/L)\n",
    "\n",
    "            #optimal subnet\n",
    "            net.one2m.weight.data[0:min_num_nuerons] = torch.tensor((w*γ).reshape(min_num_nuerons,1)) \n",
    "            net.one2m.bias.data[0:min_num_nuerons] = torch.tensor(b*γ)   \n",
    "            net.last.weight.data[0][0:min_num_nuerons]= torch.tensor(np.sign(α)*γ)\n",
    "\n",
    "        if L==4:  \n",
    "            if opt:\n",
    "                xj1 = 0\n",
    "                xj2 = 4\n",
    "                xj3 = 5 \n",
    "                α = 1\n",
    "            else:\n",
    "                if move0toneg1 and not initkinkat4:\n",
    "                    xj1 = -1\n",
    "                    α = 1\n",
    "                else:\n",
    "                    xj1 = 0\n",
    "                    α = 3/2\n",
    "                xj2 = 2\n",
    "                xj3 = 2 \n",
    "\n",
    "            #s, j found in 1DNNs.ipynb examples\n",
    "            s= np.array((-1,-1,1)) #l=3,k=0 to get ramp\n",
    "\n",
    "            #a found from reconstruction theorem\n",
    "            a1 = 2*xj2 - xj1 # R(xj1,xj2) \n",
    "            a2 = max(s[0]*(xj2-a1),0)  #4\n",
    "            a3 = max( s[1]*(max(s[0]*(xj3-a1),0)-a2) , 0)\n",
    "            a = np.array((a1,a2,a3)) \n",
    "\n",
    "            #rescaling\n",
    "            if initrescale:\n",
    "                γ = abs(α)**(1/L) #1\n",
    "                biasγ = np.array([γ**(i+1) for i in range(L)]) #over layers\n",
    "            else:\n",
    "                γ=1\n",
    "\n",
    "            #optimal subnet\n",
    "            w = s*γ #1 neuron weight over layers \n",
    "            b = -1*s*a*biasγ[:-1] #1 neuron bias over layers\n",
    "            net.one2one1.weight.data[0]=torch.tensor([w[0]])  \n",
    "            net.one2one1.bias.data[0]=torch.tensor(b[0])\n",
    "            net.one2one2.weight.data[0]=torch.tensor([w[1]])\n",
    "            net.one2one2.bias.data[0]=torch.tensor(b[1])\n",
    "            net.one2m.weight.data[0]=torch.tensor([w[2]])\n",
    "            net.one2m.bias.data[0]=torch.tensor(b[2])\n",
    "            if initrescale:\n",
    "                net.last.weight.data[0][0]=torch.tensor([np.sign(α)*γ])\n",
    "            else:\n",
    "                net.last.weight.data[0][0]=torch.tensor([α])\n",
    "            net.last.bias.data[0]=torch.tensor([0*biasγ[3]])\n",
    "\n",
    "    return net\n",
    "    \n",
    "    \n",
    "def train(net, num_epochs, ds_train, β, use_scheduler=False, opt_method='Adam'):\n",
    "    \n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    if use_scheduler: \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=200, verbose=True, factor=0.5, eps=1e-12)\n",
    "\n",
    "    if opt_method == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.5*1e-2, weight_decay=1e-4)\n",
    "    elif opt_method == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.5*1e-4, momentum=0.9)\n",
    "\n",
    "    loss_vec=np.zeros((num_epochs))\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        loss_print = 0.0\n",
    "        for i, data in enumerate(ds_train):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs=inputs.to(\"cpu\").to(torch.float64)\n",
    "            labels=labels.to(\"cpu\").to(torch.float64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.reshape(-1,1))\n",
    "            non_bias_params = []\n",
    "\n",
    "            #add l_L penalty\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'bias' not in name:\n",
    "                    non_bias_params.append(param)\n",
    "            lL_penalty = β* sum([(torch.pow(p,L)).sum() for p in non_bias_params]) #ez. usual regularization is l2 norm, not l_L norm^L\n",
    "            loss = loss + lL_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_vec[epoch]=loss.item()\n",
    "            # print statistics\n",
    "            loss_print = loss.item()\n",
    "            if num_epochs > 5:\n",
    "                if epoch % (num_epochs//5)  == 0: \n",
    "                    print(f'[{epoch + 1}, {i + 1:1d}] loss: {loss_print :.7f}') #ez changed .5 to .6\n",
    "            if use_scheduler: \n",
    "                scheduler.step(loss_print) \n",
    "\n",
    "    print('Finished Training')\n",
    "    return net, loss_vec\n",
    "\n",
    "\n",
    "def nonconvex_nn(L,opt,move0toneg1,debug=False,verbose=False,initkinkat4=True,par=False):\n",
    "    setup_dic = setup(L=L, opt=opt, move0toneg1=move0toneg1)\n",
    "    X= setup_dic['X']\n",
    "    y= setup_dic['y']\n",
    "    β= setup_dic['β']\n",
    "    m_L= setup_dic['m_L']\n",
    "    seed=setup_dic['seed']\n",
    "    num_epochs= setup_dic['num_epochs']\n",
    "    min_num_nuerons= setup_dic['min_num_nuerons']\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if debug:\n",
    "        m_L = min_num_nuerons #3 if L=2 and opt\n",
    "        num_epochs = 0\n",
    "        verbose = True\n",
    "        init_perfect = True\n",
    "        custom_init = True\n",
    "\n",
    "    ds_train, ds_test = prepTrainTestData(X=X,y=y)\n",
    "\n",
    "    net = DeepNarrowNet1D(m_L=m_L, L=L, par=par).to(\"cpu\") \n",
    "    net = initnn(net=net, L=L, opt=opt, initkinkat4=initkinkat4, move0toneg1=move0toneg1, min_num_nuerons=min_num_nuerons, debug=debug)\n",
    "    net = net.to(torch.float64)\n",
    "    if verbose:\n",
    "        print('net=',net) \n",
    "        net.apply(print_weights)\n",
    "\n",
    "    net, loss_vec = train(net=net, num_epochs=num_epochs, ds_train=ds_train, β=β)\n",
    "    if num_epochs>1:\n",
    "        print('objective = ', loss_vec[-1])\n",
    "    \n",
    "    return net, loss_vec, X, y,  num_epochs\n",
    "\n",
    "def plotresults(X,y,L):\n",
    "    noncvxtitle = 'training with Adam'\n",
    "    linewidth=3.0\n",
    "    markersize=100\n",
    "    width=6\n",
    "    height=3\n",
    "    datalabel='$(x_n,y_n)$'\n",
    "    nnlabel = 'net'\n",
    "\n",
    "    xaxislim = [X[0]-1,X[-1]+1]\n",
    "    xaxis = np.linspace(min(xaxislim)+1,max(xaxislim)-1,int(max(xaxislim)-min(xaxislim)-1))\n",
    "    Xtest=torch.tensor(np.linspace(X[0]-0.5,X[-1]+0.5,1000).reshape(-1,1)).to(\"cpu\").to(torch.float64) #old data\n",
    "\n",
    "    if opt:\n",
    "        yaxislim = [-0.5,5.5]\n",
    "    else:\n",
    "        yaxislim = [-0.2,3.2]\n",
    "\n",
    "    out1=net(Xtest).detach().to(\"cpu\").numpy()\n",
    "    xvals = Xtest.cpu().numpy()\n",
    "\n",
    "\n",
    "    ### plot\n",
    "    plt.figure(figsize=(width,height))\n",
    "    plt.scatter(X.reshape(-1),y, marker=\"o\", color=\"red\", label = datalabel, s=markersize)\n",
    "    plt.plot(Xtest.cpu().numpy(),out1, label = nnlabel, color='blue',linewidth=linewidth)\n",
    "\n",
    "    plt.xlim(xaxislim)\n",
    "    plt.ylim(yaxislim)\n",
    "    if opt and L==2:\n",
    "        plt.title(noncvxtitle)\n",
    "\n",
    "    if not opt and L==3:\n",
    "        plt.title(noncvxtitle)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(xaxis[:,0], labels=xaxis[:,0].astype(int))\n",
    "\n",
    "    if L<4:\n",
    "        plt.xticks([])\n",
    "    plt.ylabel(str(L)+' layers')\n",
    "    plt.legend()\n",
    "\n",
    "    kink_index = min([i for i in range(len(out1)) if np.abs(out1[i]-3.0)<1e-2])\n",
    "    kink = xvals[kink_index][0]\n",
    "    print('kink = ', kink)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a47f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L = 4\n",
    "#opt = False\n",
    "#move0toneg1 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfe0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net, loss_vec, X, y, num_epochs = nonconvex_nn(L=L,opt=opt,move0toneg1=move0toneg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbabeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.plot(np.arange(len(loss_vec[100:])), loss_vec[100:])\n",
    "#plt.xlabel('epochs')\n",
    "#plt.ylabel('training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7451461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotresults(X=X,y=y,L=L,Xtest=Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79cd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
