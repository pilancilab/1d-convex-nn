{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1709d5ae",
   "metadata": {},
   "source": [
    "# Functions for training deep narrow ReLU neural networks with Lasso\n",
    "\n",
    "- neural nets can have depth of L = 2,3,4 layers\n",
    "- run ```analyze_cvx_nn()``` with arguments $L$ specifying the depth and $dataset$ specifying one of three example data sets in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff21f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Mar 18 01:00:47 PM: Encountered unexpected exception importing solver SCS:\n",
      "ImportError('dlopen(/Users/emi/Library/Python/3.7/lib/python/site-packages/_scs_direct.cpython-37m-darwin.so, 2): Symbol not found: _aligned_alloc\\n  Referenced from: /Users/emi/Library/Python/3.7/lib/python/site-packages/scs/.dylibs/libgomp.1.dylib (which was built for Mac OS X 10.15)\\n  Expected in: /usr/lib/libSystem.B.dylib\\n in /Users/emi/Library/Python/3.7/lib/python/site-packages/scs/.dylibs/libgomp.1.dylib')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ec856",
   "metadata": {},
   "source": [
    "### Functions to solve general Lasso problem for deep narrow ReLU networks\n",
    "\n",
    "- uses the tuple $i=(s,j,k)$ to index dictionary columns (Lasso features) as defined in our paper (Section 3.1). The functions ```convertTuple2index()```  and ```convertIndex2Tuple()``` convert between i and (s,j,k).\n",
    "\n",
    "- recursive definition of Lasso features is used (Appendix C) instead of equivalent functional definition (Section 3.1). This is to make the code simple and verify the equivalences of the two definitions. The function ```reludic()``` computes the $(s,j,k)^{th}$ Lasso feature at a point $x$.\n",
    "\n",
    "- The function ```lasso_nn_deepnarrow()``` solves the Lasso problem with cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5b2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def fixindex(index):\n",
    "    return index-1\n",
    "\n",
    "def s2index(s):\n",
    "    #s is array with s[i] is -1 or 1\n",
    "    arraylen = len(s)\n",
    "    ss = np.zeros((arraylen,))\n",
    "    for i in range(arraylen):\n",
    "        ss[i]=0.5*(s[i]+1) #0 or 1\n",
    "    return ss\n",
    "\n",
    "def sindex2s(s):\n",
    "    #s is array with s[i] is 0 or 2\n",
    "    arraylen = len(s)\n",
    "    ss = np.zeros((arraylen,))\n",
    "    for i in range(arraylen):\n",
    "        ss[i]=2*s[i]-1 #-1 or 1\n",
    "    return ss   \n",
    "\n",
    "def undofixindex(index):\n",
    "    return index+1\n",
    "\n",
    "def reflect(a1, a2):\n",
    "    return 2*a2-a1\n",
    "\n",
    "def reludic(l,s,j,k,data,x): \n",
    "    #x is a scalar. k is a scalar. data is the sorted data matrix X. s and j are arrays of length l-1. l is a scalar: 2,3 or 4\n",
    "    if l==1:\n",
    "        return x\n",
    "    \n",
    "    l = l-1 \n",
    "    if l==1 and k==1:\n",
    "        a=reflect(data[j[fixindex(1)]],data[j[fixindex(2)]])[0]\n",
    "    else:\n",
    "        a = data[j[fixindex(l)]][0] #[0] for the way makedata outputs data\n",
    "            \n",
    "    xterm = reludic(l,s,j,k,data,x)\n",
    "    aterm = reludic(l,s,j,k,data,a)\n",
    "    sterm = s[fixindex(l)]\n",
    "          \n",
    "    out = relu(sterm*(xterm-aterm))\n",
    "    \n",
    "    return out\n",
    "\n",
    "def convertTuple2index(s,j,k,N):\n",
    "    #s is array of -1, 1. j is array of 0,...,N-1's. k is 0 or 1. \n",
    "    index=0\n",
    "    arraylen = len(s) #=length(j)\n",
    "    ss = s2index(s)\n",
    "    \n",
    "    #add j\n",
    "    for i in range(arraylen):\n",
    "        index = index + j[i]*(N**i)\n",
    "        \n",
    "    #add s\n",
    "    for i in range(arraylen):\n",
    "        index = index + ss[i]*(2**i)*(N**arraylen) \n",
    "        \n",
    "    #add k\n",
    "    index = index + k*(N**arraylen)*(2**arraylen)\n",
    "    \n",
    "    #zero-index\n",
    "    return index\n",
    "\n",
    "def convertIndex2Tuple(index,N,arraylen):\n",
    "    #arraylen is L-1, so 1,2,or 3\n",
    "    \n",
    "    s = np.zeros((arraylen,))\n",
    "    j = np.zeros((arraylen,))\n",
    "    \n",
    "    #get k\n",
    "    q, mod = divmod(index, (N**arraylen)*(2**arraylen))\n",
    "    k=q\n",
    "    \n",
    "    #get s\n",
    "    for i in range(arraylen):\n",
    "        q, mod =divmod(mod, (N**arraylen)*(2**(arraylen-i-1)))\n",
    "        s[arraylen-i-1]=q   \n",
    "     \n",
    "    #get j\n",
    "    for i in range(arraylen):\n",
    "        q, mod = divmod(mod, (N**(arraylen-i-1)))\n",
    "        j[arraylen-i-1]=q\n",
    "            \n",
    "    s = sindex2s(s) \n",
    "    s = s.astype(int)\n",
    "    j = j.astype(int)\n",
    "    \n",
    "    return s, j, k\n",
    "\n",
    "def lasso_nn_deepnarrow(L, X_train, y_train, X_test, findoptset=False, optval=[], β = 1e-7, activation_func = relu, verbose=False):\n",
    "    activation_func = relu\n",
    "    num_samples = len(X_train)\n",
    "    num_bidual_var = (2**(L-1))*(num_samples**(L-1))\n",
    "    if L==4:\n",
    "        num_bidual_var=2*num_bidual_var #k=1 are the highest indices \n",
    "    \n",
    "    #define variables z and ξ and lasso cost for bidual\n",
    "    z=cp.Variable((num_bidual_var,1)) #first argument dimension d=1\n",
    "    ξ=cp.Variable((1,1)) #bias term\n",
    "    \n",
    "    A=np.zeros((num_samples,num_bidual_var))\n",
    "    arraylen = L-1\n",
    "    for i in range(num_bidual_var):\n",
    "        s,j,k=convertIndex2Tuple(index=i,N=num_samples,arraylen=arraylen)\n",
    "        A[:,i]=np.squeeze(reludic(l=L,s=s,j=j,k=k,data=X_train,x=X_train))\n",
    "        \n",
    "    cost = 0.5*cp.sum_squares(y_train-A@z-ξ*np.ones((num_samples,1)))+β*cp.norm(z,1)\n",
    "    constraints = []\n",
    "    \n",
    "    if findoptset:\n",
    "        constraints = [cost<=optval]\n",
    "        cost = cp.sum_squares(z)\n",
    "        \n",
    "    \n",
    "    #set tolerances as best as possible\n",
    "    params = {\n",
    "        #\"MSK_IPAR_NUM_THREADS\": 8,\n",
    "        #\"MSK_IPAR_INTPNT_MAX_ITERATIONS\": 100,\n",
    "        #\"MSK_IPAR_OPTIMIZER\": 0, # auto 0, interior point 1, conic 2\n",
    "        \"MSK_DPAR_INTPNT_CO_TOL_REL_GAP\": 1e-12,\n",
    "        #\"MSK_DPAR_INTPNT_TOL_PSAFE\": 1e-8,\n",
    "        #\"MSK_IPAR_OPTIMIZER\": \"free\",\n",
    "        #\"MSK_IPAR_INTPNT_SOLVE_FORM\": 1\n",
    "      }\n",
    "   \n",
    "    #solve bidual problem \n",
    "    prob=cp.Problem(cp.Minimize(cost),constraints)\n",
    "    \n",
    "    prob.solve(solver=cp.MOSEK,warm_start=True,verbose=verbose,mosek_params=params)\n",
    "    cvx_opt=prob.value\n",
    "    mstar = np.sum(np.abs(z.value)>0) #number of neurons\n",
    "   \n",
    "    if prob.status != \"optimal\":\n",
    "        print(\"Convex: Status convex: \",prob.status)\n",
    "    if verbose:\n",
    "        print(\"2-layer convex program objective value: \",cvx_opt)\n",
    "        print(\"opt bidual var z = \", np.round(z.value, 3))\n",
    "        print(\"opt bidual bias var xi = \", ξ.value)\n",
    "        print(\"number of neurons (m*): \", mstar)\n",
    "        \n",
    "    return A, z.value, ξ.value, cvx_opt\n",
    "\n",
    "\n",
    "def makedata(X_train, y_train, include_train_in_test=False):\n",
    "    #sort X_train in descending order\n",
    "    #reorder labels y in the same order\n",
    "    #make testing data to have similar range as X_train\n",
    "    \n",
    "    sort_indices = np.argsort(-1*X_train) #descend!\n",
    "    X_train = X_train[sort_indices] #sort in descending order (for makeA())\n",
    "    X_train = np.expand_dims(X_train, axis=1) #reshape to n x 1 vector\n",
    "    num_samples_train = len(X_train)\n",
    "\n",
    "    y_train = y_train[sort_indices] #sort y in the same order as x\n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    X_test = np.linspace(np.min(X_train)-1,np.max(X_train)+1,100)\n",
    "    if include_train_in_test:\n",
    "        X_test = np.hstack((X_test, X_train.ravel())) #include train in test data\n",
    "    X_test = np.expand_dims(X_test, axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc6ece",
   "metadata": {},
   "source": [
    "### Functions to generate and plot theoretical solutions to the min norm version of Lasso for examples given in our paper\n",
    "\n",
    "- used when the $dataset$ argument in ```analyze_cvx_nn()``` is set to \"opt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bab392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramp(a,b,x):\n",
    "    if x <= a:\n",
    "        return 0\n",
    "    if x<= b:\n",
    "        return x-a\n",
    "    \n",
    "    return b-a\n",
    "\n",
    "def rampvec(a,b,x):\n",
    "    out = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        out[i]=ramp(a=a,b=b,x=x[i])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def get_features(L,x):\n",
    "    if L==4:\n",
    "        features = [rampvec(5,8,x)]\n",
    "        coefs = [1]\n",
    "    if L==3:\n",
    "        features = [rampvec(5,7,x),rampvec(5,10,x)]\n",
    "        coefs = [2/3,1/3]\n",
    "    if L==2:\n",
    "        features = [rampvec(5,np.inf,x),rampvec(7,np.inf,x),rampvec(10,np.inf,x)]\n",
    "        coefs = [1,-2/3,-1/3]\n",
    "        \n",
    "    return features, coefs\n",
    "\n",
    "def datafuncvec(x,L,verbose=False):\n",
    "    feats, coefs = get_features(L=L,x=x)\n",
    "    out = np.zeros_like(feats[0])\n",
    "    for i, feat in enumerate(feats):\n",
    "        if verbose:\n",
    "            print('feat=',feat,'coef=',coefs[i],'out=',out)\n",
    "        out = out + coefs[i]*feat\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30ec0f",
   "metadata": {},
   "source": [
    "### Function called to produce figures in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cvx_nn(L, dataset, save, figtitle, labelyaxis=False):\n",
    "    #dataset can be 'zshape', 'bounce', or 'opt'. It specifies the training samples X_train and labels y_train, and plotting parameters.\n",
    "    \n",
    "    #plotting constants\n",
    "    cvxtitle = 'training with Lasso'\n",
    "    linewidth=3.0\n",
    "    markersize=100\n",
    "    width=6\n",
    "    height=3\n",
    "    datalabel='$(x_n,y_n)$'\n",
    "    nnlabel = 'net'\n",
    "\n",
    "    #get setup\n",
    "    if dataset == 'zshape':\n",
    "        X_train = np.array((0,2,6,7)) #training samples. \n",
    "        y_train = np.array((0,0,3,3)) #labels\n",
    "        yaxislim = [min(y_train)-0.2,max(y_train)+0.2]\n",
    "        xaxislim = [min(X_train)-1,max(X_train)+1]\n",
    "        xaxis = np.linspace(min(xaxislim)+1,max(xaxislim)-1,int(max(xaxislim)-min(xaxislim)-1))\n",
    "        plottheory=False\n",
    "        xlimfeat = [-1,8.5]\n",
    "        ylimfeat = [-0.5,6.5]\n",
    "        xaxisfeat = np.arange(9)\n",
    "        β=1e-8\n",
    "        \n",
    "    elif dataset == 'bounce':\n",
    "        X_train = np.array((0,2,6,7)) \n",
    "        y_train = np.array((0,0,3,7))\n",
    "        yaxislim = [-5.2,12]\n",
    "        xaxislim = [-1,8.5]\n",
    "        xaxis = np.arange(9)\n",
    "        plottheory=False\n",
    "        xlimfeat = [-1,8.5]\n",
    "        ylimfeat = [-0.5,6.5]\n",
    "        xaxisfeat = np.arange(9)\n",
    "        β=1e-8\n",
    "        \n",
    "    elif dataset == 'opt':\n",
    "        X_train = np.array((0,4,5,7,10,11)) \n",
    "        y_train = np.array((0,0,0,2,3,3))\n",
    "        xaxislim = [-1.5,14]\n",
    "        yaxislim = [-0.5,5.5]\n",
    "        xaxis = np.linspace(min(xaxislim)+1,max(xaxislim)-1,int(max(xaxislim)-min(xaxislim)-1))\n",
    "        plottheory=True #for this example in the paper, we plot the theoretical solution to the min norm problem\n",
    "        xlimfeat = xaxislim\n",
    "        ylimfeat = yaxislim\n",
    "        xaxisfeat = np.array((0,2,4,6,8,10,12))\n",
    "        β=1e-7\n",
    "    \n",
    "    #train\n",
    "    num_samples = len(X_train)\n",
    "    X_train, y_train, X_test = makedata(X_train, y_train)\n",
    "    if not plottheory:\n",
    "        A, z, ξ, obj=lasso_nn_deepnarrow(L=L, X_train=X_train, y_train=y_train, X_test=X_test, β = β) #needed β=1e-8 or less\n",
    "        print('optimal objective = ', obj)\n",
    "\n",
    "    #get nn predictions\n",
    "    numtestpts = 100\n",
    "    if dataset == 'opt':\n",
    "         x= np.linspace(start=min(xaxislim)+1,stop=max(xaxislim)-1,num=numtestpts).reshape((numtestpts,1))\n",
    "    else:\n",
    "        x=np.linspace(min(X_train)-0.5,max(X_train)+0.5,numtestpts).reshape((numtestpts,1))\n",
    "   \n",
    "    \n",
    "    if not plottheory:\n",
    "        test_val = ξ*np.ones((numtestpts,1))\n",
    "        for i in range(len(A[0,:])):\n",
    "            s,j,k=convertIndex2Tuple(index=i,N=num_samples,arraylen=L-1)\n",
    "            test_val = test_val + z[i]*(reludic(l=L,s=s,j=j,k=k,data=X_train,x=x))\n",
    "    else:\n",
    "        test_val = datafuncvec(x=x,L=L)   \n",
    "\n",
    "    #plot predictions\n",
    "    plt.figure(figsize=(width,height))\n",
    "    plt.scatter(X_train, y_train, label = datalabel, marker=\"o\", color=\"red\", s=markersize)\n",
    "    plt.plot(x,test_val, label = nnlabel,  linewidth=linewidth, color=\"blue\")\n",
    "    plt.xlim(xaxislim)\n",
    "    plt.ylim(yaxislim)\n",
    "    plt.legend()\n",
    "    if (L==2) or (L==3 and dataset=='zshape'):\n",
    "        plt.title(cvxtitle)\n",
    "        plt.xticks([])\n",
    "    elif L<4 and dataset == 'opt':\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        ax.set_xticks(xaxis, labels=xaxis.astype(int))\n",
    "    if not labelyaxis:\n",
    "        plt.yticks([])\n",
    "    if dataset=='bounce':\n",
    "        plt.ylabel(str(L)+\" layers\")\n",
    "    if dataset=='opt' and L==4:\n",
    "        ax = plt.gca()\n",
    "        ax.set_xticks(xaxisfeat, labels=xaxisfeat.astype(int))\n",
    "\n",
    "        \n",
    "    if save:\n",
    "        plt.savefig(figtitle + \"_pred.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    if dataset=='zshape':\n",
    "        return\n",
    "\n",
    "    #get nonzero features and plot\n",
    "    plt.figure(figsize=(width,height))\n",
    "    if L==2:\n",
    "        plt.title('selected features')\n",
    "    numfeature=0\n",
    "\n",
    "    if plottheory:\n",
    "        features, coefs = get_features(L,x)\n",
    "        for i, feat in enumerate(features):\n",
    "            coefstr = str(Fraction(coefs[i]).limit_denominator(10))\n",
    "            plt.plot(x,feat,label='$z_{i_' + str(i+1) + '}^*='+ coefstr + '$',  linewidth=linewidth)\n",
    "    else:\n",
    "        z=np.squeeze(z)\n",
    "        nonzero_indices = np.nonzero(z)\n",
    "        for i in nonzero_indices[0]:\n",
    "            if np.abs(z[i])>0.05:\n",
    "                s,j,k=convertIndex2Tuple(index=i,N=num_samples,arraylen=L-1)\n",
    "                print('s,j,k=',s,j,k)\n",
    "                out = reludic(l=L,s=s,j=j,k=k,data=X_train,x=x)\n",
    "                if numfeature==0:\n",
    "                    col = \"yellow\"\n",
    "                else:\n",
    "                    col=\"green\"\n",
    "                plt.plot(x,out,label='$z_{i_' + str(numfeature+1) + '}^*='+str(round(z[i],2)) + '$', color=col,  linewidth=linewidth)\n",
    "                numfeature=numfeature+1\n",
    "\n",
    "    plt.xlim(xlimfeat)\n",
    "    plt.ylim(ylimfeat)\n",
    "    if L<4:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        ax.set_xticks(xaxisfeat, labels=xaxisfeat)\n",
    "    if dataset == 'opt':\n",
    "        plt.yticks([])\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(figtitle + \"_feat.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508ebf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_cvx_nn(L=3, dataset='opt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
